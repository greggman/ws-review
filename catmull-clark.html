<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>Catmull-Clark Subdivision Surfaces</title>
  </head>

  <body>
    <canvas width="512" height="512"></canvas>
    <script type="module">
      // inspiration: https://webgpufundamentals.org/webgpu/lessons/webgpu-fundamentals.html
      async function main() {
        const adapter = await navigator.gpu?.requestAdapter();
        const device = await adapter?.requestDevice();
        if (!device) {
          fail("Fatal error: Device does not support WebGPU.");
          return;
        }

        /** The following tables are precomputed (on the CPU): Niessner 2012:
         * "Feature-adaptive rendering involves a CPU preprocessing step, as well as a
         * GPU runtime component. Input to our algorithm is a base control mesh
         * consisting of vertices and faces, along with optional data consisting
         * of semisharp crease edge tags and hierarchical details. In the CPU
         * preprocessing stage, we use these data to construct tables containing
         * control mesh indices that drive our feature adaptive subdivision process.
         * Since these subdivision tables implicitly encode mesh connectivity, no
         * auxiliary data structures are needed for this purpose. A unique table
         * is constructed for each level of subdivision up to a prescribed maximum,
         * as well as final patch control point index buffers as described in
         * Section 3.2. The base mesh, subdivision tables, and patch index data are
         * uploaded to the GPU, one time only, for subsequent runtime processing.
         * The output of this phase depends only on the topology of the base mesh,
         * crease edges, and hierarchical detail; it is independent of the geometric
         * location of the control points." */

        /** Initial test is a 5-face rectangular pyramid, which has 23 resulting elements
         * after a single subdivision. Only the first 5 points must be populated in
         * the vertices buffer. */

        const vertices_size = 23;
        const vertices_object_size = 4; // float4s (but ignore w coord for now)
        // float3s were fraught with peril (padding)
        const vertices = new Float32Array(vertices_size * vertices_object_size);

        // prettier-ignore
        const base_vertex_positions = new Float32Array([
          0, 0, 1, 1,   // vertex 0
          -1, 1, 0, 1,  // vertex 1
          -1, -1, 0, 1, // vertex 2
          1, -1, 0, 1,  // vertex 3
          1, 1, 0, 1,   // vertex 4
        ]); // use this ONLY to populate (part of) the vertex buffer
        for (var i = 0; i < base_vertex_positions.length; i++) {
          vertices[i] = base_vertex_positions[i];
        }
        const base_vertex_count =
          base_vertex_positions.length / vertices_object_size;
        var vertices_write_ptr = base_vertex_count;
        // never use base_vertex_positions again

        // Q: Is a flattened 1D array the right way to represent base faces?
        // prettier-ignore
        const base_faces = new Uint32Array([
          0, 1, 2, // vertices in face 0
          0, 2, 3, //         ... face 1
          0, 3, 4, //         ... face 2
          0, 4, 1, //         ... face 3
          4, 3, 2, 1, //      ... face 4
        ]);
        const base_face_valence = new Uint32Array([3, 3, 3, 3, 4]);
        // base_face_offset is exclusive_scan('+', base_face_valence)
        // TODO: compute that scan in a compute shader
        const base_face_offset = new Uint32Array([0, 3, 6, 9, 12]);
        const base_faces_count = base_face_valence.length;

        // prettier-ignore
        const base_edges = new Uint32Array([
          0, 5, 2, 6,
          1, 5, 0, 8,
          2, 5, 1, 9,
          0, 6, 3, 7,
          3, 6, 2, 9,
          0, 7, 4, 8,
          4, 7, 3, 9,
          1, 8, 4, 9,
        ]);
        const edges_object_size = 4; // (two faces, two edges)
        const base_edges_count = base_edges.length / edges_object_size;

        /** (1) Calculation of face points
         * Number of faces: base_face_valence.length
         * for each face: new face point = centroid(vertices of current face)
         * Pseudocode:   (note math operations are on vec3f's)
         * parallel for i in [0 .. base_face_valence.length]:
         *   new_faces[i] = [0,0,0]
         *   for j in [base_face_offset[i] .. base_face_offset[i] + base_face_valence[i]]:
         *     new_faces[i] += base_vertex_positions[base_faces[j]
         *   new_faces[i] /= base_face_valence[i]
         */

        const facePointsModule = device.createShaderModule({
          label: "face points module",
          code: /*wgsl*/ `
        /* input + output */
        @group(0) @binding(0) var<storage, read_write> vertices: array<vec3f>;
        /* input */
        @group(0) @binding(1) var<storage, read> base_faces: array<u32>;
        @group(0) @binding(2) var<storage, read> base_face_offset: array<u32>;
        @group(0) @binding(3) var<storage, read> base_face_valence: array<u32>;

        /** Niessner 2012:
         * "The face kernel requires two buffers: one index buffer, whose
         * entries are the vertex buffer indices for each vertex of the face; a
         * second buffer stores the valence of the face along with an offset
         * into the index buffer for the first vertex of each face."
         *
         * implementation above: "index buffer" is base_faces
         *                       "valence of the face" is base_face_valence
         *                       "offset into the index buffer" is base_face_offset
         */

        @compute @workgroup_size(1) fn facePointsKernel(
          @builtin(global_invocation_id) id: vec3u) {
            let i = id.x;
            /* TODO: exit if my index is larger than the size of the input */

            let out = i + ${vertices_write_ptr};
            vertices[out] = vec3f(0,0,0);
            for (var j: u32 = base_face_offset[i]; j < base_face_offset[i] + base_face_valence[i]; j++) {
              let face_vertex = base_faces[j];
              vertices[out] += vertices[face_vertex];
            }
            vertices[out] /= f32(base_face_valence[i]);
            // TODO: decide on vec3f or vec4f and set w if so
        }
      `,
        });

        vertices_write_ptr += base_faces_count;

        /** output vertices from face kernel, for debugging:
         * [-0.6666666865348816, 0, 0.3333333432674408, 0]
         * [0, -0.6666666865348816, 0.3333333432674408, 0]
         * [0.6666666865348816, 0, 0.3333333432674408, 0]
         * [0, 0.6666666865348816, 0.3333333432674408, 0]
         * [0, 0, 0, 0]
         */

        /** (2) Calculation of edge points
         * Number of edges: base_edges.length
         * for each edge: new edge point = average(2 neighboring face points, 2 endpoints of edge)
         * Pseudocode:   (note math operations are on vec3f's)
         * parallel for i in [0 .. ?.length]:
         *   new_edges[i] = 0.25 * ( vertices[edge_id] + vertices[edge_id + 1] +
         *                           vertices[edge_id + 2] + vertices[edge_id + 3])
         */

        const edgePointsModule = device.createShaderModule({
          label: "edge points module",
          code: /*wgsl*/ `
        /* input + output */
        @group(0) @binding(0) var<storage, read_write> vertices: array<vec3f>;
        /* input */
        @group(0) @binding(1) var<storage, read> base_edges: array<vec4u>;

        /** "Since a single (non-boundary) edge always has two incident faces and vertices,
         * the edge kernel needs a buffer for the indices of these entities."
         *
         * implementation above: "a buffer for the indices of these entities" is base_edges
         */

        @compute @workgroup_size(1) fn edgePointsKernel(
          @builtin(global_invocation_id) id: vec3u) {
            let i = id.x;
            /* TODO: exit if my index is larger than the size of the input */

            let out = i + ${vertices_write_ptr};
            let edge_id = i;
            vertices[out] = vec3f(0,0,0);
            for (var j: u32 = 0; j < 4; j++) {
              vertices[out] += vertices[base_edges[edge_id][j]];
            }
            vertices[out] *= 0.25;
        }
      `,
        });

        /** output "edge" vertices from edge kernel, for debugging
         * 10 | -0.4166666865348816, -0.4166666865348816, 0.4166666865348816, 0
         * 11 | -0.4166666865348816, 0.4166666865348816, 0.4166666865348816, 0
         * 12 | -0.6666666865348816, 0, 0.0833333358168602, 0
         * 13 | 0.4166666865348816, -0.4166666865348816, 0.4166666865348816, 0
         * 14 | 0, -0.6666666865348816, 0.0833333358168602, 0
         * 15 | 0.4166666865348816, 0.4166666865348816, 0.4166666865348816, 0
         * 16 | 0.6666666865348816, 0, 0.0833333358168602, 0
         * 17 | 0, 0.6666666865348816, 0.0833333358168602, 0
         */

        /** (3) Calculation of vertex points
         * This is more involved. References:
         * - https://www.rorydriscoll.com/2008/08/01/catmull-clark-subdivision-the-basics/
         * - https://en.wikipedia.org/wiki/Catmull%E2%80%93Clark_subdivision_surface
         * Big picture:
         * - n is valence of this point
         * - F is the average of all neighboring faces (new face points)
         * - Ve is the average of the other endpoint of all incident edges
         *   - F and Ve are just listed in the base_vertices table
         * - V is this vertex
         *   - Output is (F + Ve + (n-2) V) / n
         * - If F and Ve points are f_0, f_1, Ve_0, ...:
         *   - Output is [(f_0 + f_1 + ... + Ve_0 + Ve_1 + ...) / n _ (n-2) V] / n
         * Number of vertex points: base_vertex_valence.length
         * Pseudocode:   (note math operations are on vec3f's)
         * parallel for i in [0 .. base_vertex_valence.length]:
         *   new_vertices[i] = [0,0,0]
         *   for j in [base_vertex_offset[i] .. base_vertex_offset[i] + base_vertex_valence[i]]:
         *     new_vertex[i] += vertices[base_vertices[j]]
         *   new_vertex[i] /= base_vertex_valence[i]
         *   new_vertex[i] += (n-2) * base_vertex_index[i]
         *   new_vertex[i] /= base_vertex_valence[i]
         */

        const vertexPointsModule = device.createShaderModule({
          label: "vertex points module",
          code: /*wgsl*/ `
        /* input + output */
        @group(0) @binding(0) var<storage, read_write> vertices: array<vec3f>;
        /* input */
        @group(0) @binding(1) var<storage, read> base_vertices: array<u32>;

        /** "We use an index buffer containing the indicies of the incident edge and
         * vertex points."
         *
         * implementation above: "a buffer for the indices of these entities" is base_edges
         */

        @compute @workgroup_size(1) fn vertexPointsKernel(
          @builtin(global_invocation_id) id: vec3u) {
            let i = id.x;
            /* TODO: exit if my index is larger than the size of the input */

            let out = i + ${vertices_write_ptr};
            let edge_id = i;
            vertices[out] = vec3f(0,0,0);
            for (var j: u32 = 0; j < 4; j++) {
              // vertices[out] += vertices[base_edges[edge_id][j]];
            }
            vertices[out] *= 0.25;
        }
      `,
        });

        const face_pipeline = device.createComputePipeline({
          label: "face points compute pipeline",
          layout: "auto",
          compute: {
            module: facePointsModule,
          },
        });

        const edge_pipeline = device.createComputePipeline({
          label: "edge points compute pipeline",
          layout: "auto",
          compute: {
            module: edgePointsModule,
          },
        });

        const vertex_pipeline = device.createComputePipeline({
          label: "vertex points compute pipeline",
          layout: "auto",
          compute: {
            module: vertexPointsModule,
          },
        });

        // create buffers on the GPU to hold data
        // read-only inputs:
        const base_faces_buffer = device.createBuffer({
          label: "base faces buffer",
          size: base_faces.byteLength,
          usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
        });
        device.queue.writeBuffer(base_faces_buffer, 0, base_faces);

        const base_edges_buffer = device.createBuffer({
          label: "base edges buffer",
          size: base_edges.byteLength,
          usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
        });
        device.queue.writeBuffer(base_edges_buffer, 0, base_edges);

        const base_face_offset_buffer = device.createBuffer({
          label: "base offset",
          size: base_face_offset.byteLength,
          usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
        });
        device.queue.writeBuffer(base_face_offset_buffer, 0, base_face_offset);

        const base_face_valence_buffer = device.createBuffer({
          label: "base valence",
          size: base_face_valence.byteLength,
          usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
        });
        device.queue.writeBuffer(
          base_face_valence_buffer,
          0,
          base_face_valence
        );

        // vertex buffer is both input and output
        const vertices_buffer = device.createBuffer({
          label: "vertex buffer",
          size: vertices.byteLength,
          usage:
            GPUBufferUsage.STORAGE |
            GPUBufferUsage.COPY_DST |
            GPUBufferUsage.COPY_SRC,
        });
        device.queue.writeBuffer(vertices_buffer, 0, vertices);

        /** and the mappable output buffer (I believe that "mappable" is the only way to read from GPU->CPU) */
        const mappable_result_buffer = device.createBuffer({
          label: "mappable result buffer",
          size: vertices.byteLength,
          usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST,
        });

        /** Set up bindGroups per compute kernel to tell the shader which buffers to use */
        const face_bindGroup = device.createBindGroup({
          label: "bindGroup for face kernel",
          layout: face_pipeline.getBindGroupLayout(0),
          entries: [
            { binding: 0, resource: { buffer: vertices_buffer } },
            { binding: 1, resource: { buffer: base_faces_buffer } },
            { binding: 2, resource: { buffer: base_face_offset_buffer } },
            { binding: 3, resource: { buffer: base_face_valence_buffer } },
          ],
        });

        const edge_bindGroup = device.createBindGroup({
          label: "bindGroup for edge kernel",
          layout: edge_pipeline.getBindGroupLayout(0),
          entries: [
            { binding: 0, resource: { buffer: vertices_buffer } },
            { binding: 1, resource: { buffer: base_edges_buffer } },
          ],
        });

        // Encode commands to do the computation
        const encoder = device.createCommandEncoder({
          label: "overall computation (face, edge) encoder",
        });
        const face_pass = encoder.beginComputePass({
          label: "face kernel compute pass",
        });
        face_pass.setPipeline(face_pipeline);
        face_pass.setBindGroup(0, face_bindGroup);
        face_pass.dispatchWorkgroups(base_vertex_count);
        face_pass.end();

        const edge_pass = encoder.beginComputePass({
          label: "edge kernel compute pass",
        });
        edge_pass.setPipeline(edge_pipeline);
        edge_pass.setBindGroup(0, edge_bindGroup);
        edge_pass.dispatchWorkgroups(base_edges_count);
        edge_pass.end();

        // Encode a command to copy the results to a mappable buffer.
        // this is (from, to)
        encoder.copyBufferToBuffer(
          vertices_buffer,
          0,
          mappable_result_buffer,
          0,
          mappable_result_buffer.size
        );

        // Finish encoding and submit the commands
        const commandBuffer = encoder.finish();
        device.queue.submit([commandBuffer]);

        // Read the results
        await mappable_result_buffer.mapAsync(GPUMapMode.READ);
        const result = new Float32Array(
          mappable_result_buffer.getMappedRange().slice()
        );
        mappable_result_buffer.unmap();

        console.log("base vertex positions", vertices);
        console.log("final vertex buffer", result);
      }

      function fail(msg) {
        // eslint-disable-next-line no-alert
        alert(msg);
      }

      main();
    </script>
  </body>
</html>
