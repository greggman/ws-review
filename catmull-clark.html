<!doctype html>

<html>

<head>
  <meta charset="utf-8">
  <title>Catmull-Clark Subdivision Surfaces</title>
</head>

<body>
  <canvas width="512" height="512"></canvas>
  <script type="module">
    // inspiration: https://webgpufundamentals.org/webgpu/lessons/webgpu-fundamentals.html
    async function main() {
      const adapter = await navigator.gpu?.requestAdapter();
      const device = await adapter?.requestDevice();
      if (!device) {
        fail('Fatal error: Device does not support WebGPU.');
        return;
      }

      // The following tables are precomputed (on the CPU): Niessner 2012:
      // "Feature-adaptive rendering involves a CPU preprocessing step, as well as a
      // GPU runtime component. Input to our algorithm is a base control mesh
      // consisting of vertices and faces, along with optional data consisting
      // of semisharp crease edge tags and hierarchical details. In the CPU
      // preprocessing stage, we use these data to construct tables containing
      // control mesh indices that drive our feature adaptive subdivision process.
      // Since these subdivision tables implicitly encode mesh connectivity, no
      // auxiliary data structures are needed for this purpose. A unique table
      // is constructed for each level of subdivision up to a prescribed maximum,
      // as well as final patch control point index buffers as described in
      // Section 3.2. The base mesh, subdivision tables, and patch index data are
      // uploaded to the GPU, one time only, for subsequent runtime processing.
      // The output of this phase depends only on the topology of the base mesh,
      // crease edges, and hierarchical detail; it is independent of the geometric
      // location of the control points.""

      // Initial test is a 5-face rectangular pyramid, which has 23 resulting
      // elements after a single subdivision. Only the first 5 points must be
      // populated.

      const vertices_size = 23;
      const vertices_object_size = 4; // float4s (but ignore w coord for now)
                                           // float3s were fraught with peril (padding)
      const vertices = new Float32Array(vertices_size * vertices_object_size);

      const base_vertex_positions = new Float32Array([
        0, 0, 1, 1,   // vertex 0
        -1, 1, 0, 1,  // vertex 1
        -1, -1, 0, 1, // vertex 2
        1, -1, 0, 1,  // vertex 3
        1, 1, 0, 1    // vertex 4
      ]); // use this ONLY to populate (part of) the vertex buffer
      for (var i = 0; i < base_vertex_positions.length; i++) {
        vertices[i] = base_vertex_positions[i];
      }
      const base_vertex_count = base_vertex_positions.length / vertices_object_size;
      var vertices_write_ptr = base_vertex_count;
      // never use base_vertex_positions again


      // Q: Is a flattened 1D array the right way to represent base faces?
      const base_faces = new Uint32Array([
        0, 1, 2,   // vertices in face 0
        0, 2, 3,   //         ... face 1
        0, 3, 4,   //         ... face 2
        0, 4, 1,   //         ... face 3
        4, 3, 2, 1 //         ... face 4
      ]);
      const base_valence = new Uint32Array([3, 3, 3, 3, 4]);
      // base_offset is exclusive_scan('+', base_valence)
      // TODO: compute that scan in a compute shader
      const base_offset = new Uint32Array([0, 3, 6, 9, 12]);

      // (1) Calculation of face points
      // Number of faces: base_valence.length
      // for each face: new face point = centroid(vertices of current face)
      // Pseudocode:   (note math operations are on vec3f's)
      // parallel for i in [0 .. base_valence.length]:
      //   new_faces[i] = [0,0,0]
      //   for j in [base_offset[i] .. base_offset[i] + base_valence[i]]:
      //     new_faces[i] += base_vertex_positions[base_faces[j]
      //   new_faces[i] /= base_valence[i]

      const facePointsModule = device.createShaderModule({
        label: 'face points module',
        code: /*wgsl*/`
        /* input + output */
        @group(0) @binding(0) var<storage, read_write> vertices: array<vec3f>;
        /* input */
        @group(0) @binding(1) var<storage, read> base_faces: array<u32>;
        @group(0) @binding(2) var<storage, read> base_offset: array<u32>;
        @group(0) @binding(3) var<storage, read> base_valence: array<u32>;

        // Niessner 2012:
        // "The face kernel requires two buffers: one index buffer, whose
        // entries are the vertex buffer indices for each vertex of the face; a
        // second buffer stores the valence of the face along with an offset
        // into the index buffer for the first vertex of each face."
        //
        // implementation above: "index buffer" is base_faces
        //                       "valence of the face" is base_valence
        //                       "offset into the index buffer" is base_offset

        @compute @workgroup_size(1) fn facePointsKernel(
          @builtin(global_invocation_id) id: vec3u) {
            let i = id.x;
            /* TODO: exit if my index is larger than the size of the input */

            let out = i + ${vertices_write_ptr};
            vertices[out] = vec3f(0,0,0);
            for (var j: u32 = base_offset[i]; j < base_offset[i] + base_valence[i]; j++) {
              let face_vertex = base_faces[j];
              vertices[out] += vertices[face_vertex];
            }
            vertices[out] /= f32(base_valence[i]);
            // TODO: decide on vec3f or vec4f and set w if so
        }
      `,
      });

      // output vertices from face kernel, for debugging:
      // [-0.6666666865348816, 0, 0.3333333432674408, 0]
      // [0, -0.6666666865348816, 0.3333333432674408, 0]
      // [0.6666666865348816, 0, 0.3333333432674408, 0]
      // [0, 0.6666666865348816, 0.3333333432674408, 0]
      // [0, 0, 0, 0]

      // (2) Calculation of edge points
      // Number of edges: ?.length
      // for each edge: new edge point = average(2 neighboring face points, 2 endpoints of edge)
      // Pseudocode:   (note math operations are on vec3f's)
      // parallel for i in [0 .. ?.length]:
      //   new_edges[i] = 0.25 * (
      //
      //                         )


      const edgePointsModule = device.createShaderModule({
        label: 'edge points module',
        code: /*wgsl*/`
        /* output */
        @group(0) @binding(0) var<storage, read_write> new_faces: array<vec3f>;
        /* input */
        @group(0) @binding(1) var<storage, read> base_vertex_positions: array<vec3f>;
        /* TODO: probably switch (1) and (2) for ordering */
        @group(0) @binding(2) var<storage, read> base_faces: array<u32>;
        @group(0) @binding(3) var<storage, read> base_offset: array<u32>;
        @group(0) @binding(4) var<storage, read> base_valence: array<u32>;

        // Since a single (non-boundary) edge always has two incident faces and vertices,
        // the edge kernel needs a buffer for the indices of these entities.
        //
        // implementation above: "index buffer" is base_faces
        //                       "valence of the face" is base_valence
        //                       "offset into the index buffer" is base_offset

        @compute @workgroup_size(1) fn edgePointsKernel(
          @builtin(global_invocation_id) id: vec3u) {
        }
      `,
      });

      const pipeline = device.createComputePipeline({
        label: 'face points compute pipeline',
        layout: 'auto',
        compute: {
          module: facePointsModule,
        },
      });

      // create buffers on the GPU to hold data
      // start with inputs
      const base_faces_buffer = device.createBuffer({
        label: 'base faces buffer',
        size: base_faces.byteLength,
        usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
      });
      device.queue.writeBuffer(base_faces_buffer, 0, base_faces);

      const vertices_buffer = device.createBuffer({
        label: 'vertex buffer',
        size: vertices.byteLength,
        usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST | GPUBufferUsage.COPY_SRC,
      });
      device.queue.writeBuffer(vertices_buffer, 0, vertices);

      const base_offset_buffer = device.createBuffer({
        label: 'base offset',
        size: base_offset.byteLength,
        usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
      });
      device.queue.writeBuffer(base_offset_buffer, 0, base_offset);

      const base_valence_buffer = device.createBuffer({
        label: 'base valence',
        size: base_valence.byteLength,
        usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
      });
      device.queue.writeBuffer(base_valence_buffer, 0, base_valence);

      // and the mappable output buffer (I believe that "mappable" is the only way to read from GPU->CPU)
      const mappable_result_buffer = device.createBuffer({
        label: 'mappable result buffer',
        size: vertices.byteLength,
        usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST,
      });

      // Setup a face bindGroup to tell the shader which
      // buffers to use for the face computation
      const face_bindGroup = device.createBindGroup({
        label: 'bindGroup for face kernel',
        layout: pipeline.getBindGroupLayout(0),
        entries: [
          { binding: 0, resource: { buffer: vertices_buffer } },
          { binding: 1, resource: { buffer: base_faces_buffer } },
          { binding: 2, resource: { buffer: base_offset_buffer } },
          { binding: 3, resource: { buffer: base_valence_buffer } },
        ],
      });

      // Encode commands to do the computation
      const encoder = device.createCommandEncoder({
        label: 'face kernel encoder',
      });
      const pass = encoder.beginComputePass({
        label: 'face kernel compute pass',
      });
      pass.setPipeline(pipeline);
      pass.setBindGroup(0, face_bindGroup);
      pass.dispatchWorkgroups(base_vertex_count);
      pass.end();

      // Encode a command to copy the results to a mappable buffer.
      // this is (from, to)
      encoder.copyBufferToBuffer(vertices_buffer, 0, mappable_result_buffer, 0, mappable_result_buffer.size);

      // Finish encoding and submit the commands
      const commandBuffer = encoder.finish();
      device.queue.submit([commandBuffer]);

      // Read the results
      await mappable_result_buffer.mapAsync(GPUMapMode.READ);
      const result = new Float32Array(mappable_result_buffer.getMappedRange().slice());
      mappable_result_buffer.unmap();

      console.log('base vertex positions', vertices);
      console.log('result', result);
    }

    function fail(msg) {
      // eslint-disable-next-line no-alert
      alert(msg);
    }

    main();
  </script>
</body>

</html>